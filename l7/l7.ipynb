{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"l7\").master('local[2]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.options(header='true', inferSchema='true').csv(\"file:///root/code/datasets/PBMC_16k_RNA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = spark.read.options(header='true', inferSchema='true').csv(\"file:///root/code/datasets/PBMC_16k_RNA_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_columns = []\n",
    "for col in spark_df.columns:\n",
    "    transformed_columns.append(col.strip().replace('.', ''))\n",
    "\n",
    "spark_df = spark_df.toDF(*transformed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Assemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 06:59:14,902 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             index|            features|\n",
      "+------------------+--------------------+\n",
      "|AAGGTTCTCAGTTTGG-1|[-0.06204129,-0.2...|\n",
      "|CGGACGTAGAAACGCC-1|[-0.09675046,-0.4...|\n",
      "|GGCTCGATCCTAAGTG-1|[-0.043204147,-0....|\n",
      "|TACACGACAATAGCGG-1|[-0.056053527,-0....|\n",
      "|TCAATCTCATTCGACA-1|[-0.042632394,-0....|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Merge the features into one vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = spark_df.drop('index').columns\n",
    "feature_assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "feature_df = feature_assembler.transform(spark_df).select('index', 'features')\n",
    "feature_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+\n",
      "|             index|standardized_features|\n",
      "+------------------+---------------------+\n",
      "|AAGGTTCTCAGTTTGG-1| [-0.1244221378800...|\n",
      "|CGGACGTAGAAACGCC-1| [-0.1940304444681...|\n",
      "|GGCTCGATCCTAAGTG-1| [-0.0866447544050...|\n",
      "|TACACGACAATAGCGG-1| [-0.1124138402837...|\n",
      "|TCAATCTCATTCGACA-1| [-0.0854981191465...|\n",
      "+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='standardized_features')\n",
    "standardized_df = scaler.fit(feature_df).transform(feature_df).select('index', 'standardized_features')\n",
    "standardized_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Upon Standardized Vector with More Principal Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 06:59:44,943 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2022-07-22 06:59:44,950 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2022-07-22 07:00:42,229 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "2022-07-22 07:00:42,230 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=3, inputCol='standardized_features', outputCol='pca_features')\n",
    "pca_model = pca.fit(standardized_df)\n",
    "pca_df = pca_model.transform(standardized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explained Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02441924, 0.03537516, 0.04234139])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model.explainedVariance.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_pca_df = pca_df.join(label_df, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+--------------------+--------+\n",
      "|             index|standardized_features|        pca_features|CITEsort|\n",
      "+------------------+---------------------+--------------------+--------+\n",
      "|AAGGTTCTCAGTTTGG-1| [-0.1244221378800...|[7.55190724793356...|     ACT|\n",
      "|CGGACGTAGAAACGCC-1| [-0.1940304444681...|[-4.6778081823218...|  C-mono|\n",
      "|GGCTCGATCCTAAGTG-1| [-0.0866447544050...|[2.59406523090638...|  CD4+ T|\n",
      "|TACACGACAATAGCGG-1| [-0.1124138402837...|[5.65480016910858...|  CD4+ T|\n",
      "|TCAATCTCATTCGACA-1| [-0.0854981191465...|[4.88987145625218...|  CD8+ T|\n",
      "+------------------+---------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_pca_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-A cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|CITEsort|count|\n",
      "+--------+-----+\n",
      "| CD8+ DC|   81|\n",
      "|     iNK|  113|\n",
      "| CD4+ DC|  166|\n",
      "|     DNT|  178|\n",
      "|     mDC|  303|\n",
      "|  B cell|  414|\n",
      "| NC-mono|  537|\n",
      "|     mNK| 1057|\n",
      "|  CD8+ T| 2035|\n",
      "|  C-mono| 2313|\n",
      "|     ACT| 2952|\n",
      "|  CD4+ T| 5262|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labeled_pca_df.groupBy('CITEsort').count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divide Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'standardized_features', 'pca_features', 'CITEsort']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_pca_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        pca_features|\n",
      "+--------------------+\n",
      "|[2.59406523090638...|\n",
      "|[5.65480016910858...|\n",
      "|[4.49718949234563...|\n",
      "|[1.03337582005633...|\n",
      "|[0.13968854037425...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_pca_df.createOrReplaceTempView(\"data\")\n",
    "\n",
    "t_cell_df = spark.sql(\"SELECT pca_features FROM data WHERE CITEsort == 'CD4+ T'\")\n",
    "non_t_cell_df = spark.sql(\"SELECT pca_features FROM data WHERE CITEsort != 'CD4+ T'\")\n",
    "t_cell_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Data: change the label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [2.5940652309063856,-2.891411819704267,-1.736792233172006]),\n",
       " LabeledPoint(0.0, [5.654800169108588,-2.954814475956952,-2.129561857845239]),\n",
       " LabeledPoint(0.0, [4.497189492345637,-2.1081466516542253,-2.1047844793474755]),\n",
       " LabeledPoint(0.0, [1.0333758200563368,-2.7457468764231288,-1.0717953732629908]),\n",
       " LabeledPoint(0.0, [0.13968854037425663,-2.57990288682906,-1.1745761580036478])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "t_cell_rdd = t_cell_df.rdd.map(lambda x: LabeledPoint(0, [x[0]]))\n",
    "non_t_cell_rdd = non_t_cell_df.rdd.map(lambda x: LabeledPoint(1, [x[0]]))\n",
    "\n",
    "t_cell_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reunion and Split Data with Random Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = t_cell_rdd.union(non_t_cell_rdd)\n",
    "(training_data, test_data) = data.randomSplit([0.7, 0.3], seed=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 07:04:13,041 WARN util.Instrumentation: [412cd8cf] Initial coefficients will be ignored! Its dimensions (1, 3) did not match the expected size (1, 3)\n",
      "[Stage 64:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate: 0.23958110707416114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "training_data.cache()\n",
    "model = LogisticRegressionWithLBFGS.train(training_data, iterations=100)\n",
    "labels_and_predictions = test_data.map(lambda x: (x.label, model.predict(x.features)))\n",
    "error_rate = labels_and_predictions.filter(lambda res: res[0] != res[1]).count() / float(test_data.count())\n",
    "\n",
    "print(\"Error Rate: \" + str(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
